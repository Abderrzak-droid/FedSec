{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 11:05:08.732446: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-04 11:05:08.886488: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-04 11:05:08.886537: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-04 11:05:08.887297: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-04 11:05:08.948664: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-04 11:05:09.626154: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "import numpy as np\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# Enable eager execution (for TFF compatibility)\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_columns = [\n",
    "    \"Flow ID\", \"Src IP\", \"Src Port\", \"Dst IP\", \"Dst Port\", \"Protocol\", \"Timestamp\",\n",
    "    \"Flow Duration\", \"Total Fwd Packet\", \"Total Bwd packets\", \"Total Length of Fwd Packet\",\n",
    "    \"Total Length of Bwd Packet\", \"Fwd Packet Length Max\", \"Fwd Packet Length Min\",\n",
    "    \"Fwd Packet Length Mean\", \"Fwd Packet Length Std\", \"Bwd Packet Length Max\",\n",
    "    \"Bwd Packet Length Min\", \"Bwd Packet Length Mean\", \"Bwd Packet Length Std\",\n",
    "    \"Flow Bytes/s\", \"Flow Packets/s\", \"Flow IAT Mean\", \"Flow IAT Std\", \"Flow IAT Max\",\n",
    "    \"Flow IAT Min\", \"Fwd IAT Total\", \"Fwd IAT Mean\", \"Fwd IAT Std\", \"Fwd IAT Max\",\n",
    "    \"Fwd IAT Min\", \"Bwd IAT Total\", \"Bwd IAT Mean\", \"Bwd IAT Std\", \"Bwd IAT Max\",\n",
    "    \"Bwd IAT Min\", \"Fwd PSH Flags\", \"Bwd PSH Flags\", \"Fwd URG Flags\", \"Bwd URG Flags\",\n",
    "    \"Fwd Header Length\", \"Bwd Header Length\", \"Fwd Packets/s\", \"Bwd Packets/s\",\n",
    "    \"Packet Length Min\", \"Packet Length Max\", \"Packet Length Mean\", \"Packet Length Std\",\n",
    "    \"Packet Length Variance\", \"FIN Flag Count\", \"SYN Flag Count\", \"RST Flag Count\",\n",
    "    \"PSH Flag Count\", \"ACK Flag Count\", \"URG Flag Count\", \"CWR Flag Count\", \"ECE Flag Count\",\n",
    "    \"Down/Up Ratio\", \"Average Packet Size\", \"Fwd Segment Size Avg\", \"Bwd Segment Size Avg\",\n",
    "    \"Fwd Bytes/Bulk Avg\", \"Fwd Packet/Bulk Avg\", \"Fwd Bulk Rate Avg\", \"Bwd Bytes/Bulk Avg\",\n",
    "    \"Bwd Packet/Bulk Avg\", \"Bwd Bulk Rate Avg\", \"Subflow Fwd Packets\", \"Subflow Fwd Bytes\",\n",
    "    \"Subflow Bwd Packets\", \"Subflow Bwd Bytes\", \"FWD Init Win Bytes\", \"Bwd Init Win Bytes\",\n",
    "    \"Fwd Act Data Pkts\", \"Fwd Seg Size Min\", \"Active Mean\", \"Active Std\", \"Active Max\",\n",
    "    \"Active Min\", \"Idle Mean\", \"Idle Std\", \"Idle Max\", \"Idle Min\", \"Activity\", \"Stage\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /home/brahim/Desktop/PFE2025/Dataset/csv/enp0s3-monday-pvt.pcap_Flow.csv: (3404, 85) rows\n",
      "Loading /home/brahim/Desktop/PFE2025/Dataset/csv/enp0s3-public-wednesday.pcap_Flow.csv: (17487, 85) rows\n",
      "Loading /home/brahim/Desktop/PFE2025/Dataset/csv/enp0s3-public-thursday.pcap_Flow.csv: (9685, 85) rows\n",
      "Loading /home/brahim/Desktop/PFE2025/Dataset/csv/enp0s3-pvt-wednesday.pcap_Flow.csv: (1437, 85) rows\n",
      "Loading /home/brahim/Desktop/PFE2025/Dataset/csv/enp0s3-pvt-thursday.pcap_Flow.csv: (4113, 85) rows\n",
      "Loading /home/brahim/Desktop/PFE2025/Dataset/csv/enp0s3-tcpdump-friday.pcap_Flow.csv: (7361, 85) rows\n",
      "Loading /home/brahim/Desktop/PFE2025/Dataset/csv/enp0s3-monday.pcap_Flow.csv: (8728, 85) rows\n",
      "Loading /home/brahim/Desktop/PFE2025/Dataset/csv/enp0s3-tcpdump-pvt-friday.pcap_Flow.csv: (2618, 85) rows\n",
      "Loading /home/brahim/Desktop/PFE2025/Dataset/csv/enp0s3-pvt-tuesday.pcap_Flow.csv: (2615, 85) rows\n",
      "Loading /home/brahim/Desktop/PFE2025/Dataset/csv/enp0s3-public-tuesday.pcap_Flow.csv: (29242, 85) rows\n",
      "\n",
      "Loaded 10 files\n",
      "Combined dataset shape: (86690, 85)\n",
      "Total samples: 86690\n",
      "Features: ['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Total Fwd Packet', 'Total Bwd packets', 'Total Length of Fwd Packet', 'Total Length of Bwd Packet', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s', 'Packet Length Min', 'Packet Length Max', 'Packet Length Mean', 'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count', 'CWR Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Average Packet Size', 'Fwd Segment Size Avg', 'Bwd Segment Size Avg', 'Fwd Bytes/Bulk Avg', 'Fwd Packet/Bulk Avg', 'Fwd Bulk Rate Avg', 'Bwd Bytes/Bulk Avg', 'Bwd Packet/Bulk Avg', 'Bwd Bulk Rate Avg', 'Subflow Fwd Packets', 'Subflow Fwd Bytes', 'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'FWD Init Win Bytes', 'Bwd Init Win Bytes', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Activity', 'Stage']\n",
      "\n",
      "Stage distribution:\n",
      "Stage\n",
      "BENIGN                19454\n",
      "Benign                42458\n",
      "Data Exfiltration        15\n",
      "Establish Foothold     8604\n",
      "Lateral Movement        137\n",
      "Reconnaissance        11909\n",
      "dtype: int64\n",
      "\n",
      "Preprocessing Steps:\n",
      "Initial shape: (86690, 85)\n",
      "Dropped Flow ID column\n",
      "Missing values before handling: 345492\n",
      "Shape after handling missing values: (86690, 84)\n",
      "Shape after removing duplicates: (82578, 84)\n",
      "Dropped Activity column; only Stage will be used as label.\n",
      "Converting Src IP to numeric...\n",
      "Converting Dst IP to numeric...\n",
      "Converting Timestamp to numeric...\n",
      "\n",
      "Label Encoding Information:\n",
      "Stage classes: ['BENIGN' 'Benign' 'Data Exfiltration' 'Establish Foothold'\n",
      " 'Lateral Movement' 'Reconnaissance']\n",
      "\n",
      "Final shapes:\n",
      "X_train: (57804, 82)\n",
      "X_test: (24774, 82)\n",
      "\n",
      "Client 1:\n",
      "Samples: 11561\n",
      "Unique stages: 6\n",
      "\n",
      "Client 2:\n",
      "Samples: 11561\n",
      "Unique stages: 6\n",
      "\n",
      "Client 3:\n",
      "Samples: 11561\n",
      "Unique stages: 6\n",
      "\n",
      "Client 4:\n",
      "Samples: 11561\n",
      "Unique stages: 6\n",
      "\n",
      "Client 5:\n",
      "Samples: 11560\n",
      "Unique stages: 6\n",
      "\n",
      "Preprocessing complete!\n",
      "Training data shape: 57804 samples\n",
      "Test data shape: 24774 samples\n",
      "Stage classes: ['BENIGN' 'Benign' 'Data Exfiltration' 'Establish Foothold'\n",
      " 'Lateral Movement' 'Reconnaissance']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "def load_multiple_files(data_dir):\n",
    "    \"\"\"Load and combine multiple CSV files from a directory using only desired columns.\"\"\"\n",
    "    file_pattern = os.path.join(data_dir, \"*.csv\")\n",
    "    file_list = glob(file_pattern)\n",
    "    \n",
    "    if not file_list:\n",
    "        raise ValueError(f\"No CSV files found in {data_dir}\")\n",
    "    \n",
    "    dfs = []\n",
    "    for file in file_list:\n",
    "        # Read only the first 85 columns (or use the list of indices you prefer)\n",
    "        df = pd.read_csv(file, usecols=list(range(85)))\n",
    "        print(f\"Loading {file}: {df.shape} rows\")\n",
    "        dfs.append(df)\n",
    "    \n",
    "    combined_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "    # Select only the valid columns from our header_columns list.\n",
    "    combined_df = combined_df[header_columns]\n",
    "\n",
    "    print(f\"\\nLoaded {len(file_list)} files\")\n",
    "    print(f\"Combined dataset shape: {combined_df.shape}\")\n",
    "    print(f\"Total samples: {len(combined_df)}\")\n",
    "    print(f\"Features: {combined_df.columns.tolist()}\")\n",
    "    print(\"\\nStage distribution:\")\n",
    "    print(combined_df.groupby('Stage').size())\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocessing pipeline using only the Stage feature as the target label.\"\"\"\n",
    "    print(\"\\nPreprocessing Steps:\")\n",
    "    print(f\"Initial shape: {df.shape}\")\n",
    "    \n",
    "    # If needed, store Flow ID separately and drop it\n",
    "    if 'Flow ID' in df.columns:\n",
    "        flow_ids = df['Flow ID'].values\n",
    "        df = df.drop('Flow ID', axis=1)\n",
    "        print(\"Dropped Flow ID column\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    print(f\"Missing values before handling: {df.isna().sum().sum()}\")\n",
    "    # Fill numeric columns with median\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "    # Fill categorical columns with mode\n",
    "    categorical_cols = df.select_dtypes(exclude=[np.number]).columns\n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    \n",
    "    print(f\"Shape after handling missing values: {df.shape}\")\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"Shape after removing duplicates: {df.shape}\")\n",
    "    \n",
    "    # Drop the Activity column because we only use Stage for classification.\n",
    "    if 'Activity' in df.columns:\n",
    "        df = df.drop(columns=['Activity'])\n",
    "        print(\"Dropped Activity column; only Stage will be used as label.\")\n",
    "    \n",
    "    # Separate features and label\n",
    "    # Exclude 'Stage' from features since it is the target label.\n",
    "    feature_cols = [col for col in df.columns if col != 'Stage']\n",
    "    \n",
    "    # Convert non-numeric features (if any) to numeric.\n",
    "    for col in feature_cols:\n",
    "        if df[col].dtype == 'object':\n",
    "            print(f\"Converting {col} to numeric...\")\n",
    "            # Example: if it's an IP address, convert it to an integer representation.\n",
    "            if 'IP' in col:\n",
    "                df[col] = df[col].apply(lambda x: int(''.join([i.zfill(3) for i in str(x).split('.')])) )\n",
    "            else:\n",
    "                le = LabelEncoder()\n",
    "                df[col] = le.fit_transform(df[col])\n",
    "    \n",
    "    X = df[feature_cols].values.astype(np.float32)\n",
    "    stage = df['Stage'].values\n",
    "    \n",
    "    # Encode the Stage labels only\n",
    "    stage_encoder = LabelEncoder()\n",
    "    y_stage = stage_encoder.fit_transform(stage)\n",
    "    \n",
    "    print(\"\\nLabel Encoding Information:\")\n",
    "    print(\"Stage classes:\", stage_encoder.classes_)\n",
    "    \n",
    "    # Split dataset using only the stage label for stratification.\n",
    "    X_train, X_test, y_stage_train, y_stage_test = train_test_split(\n",
    "        X, y_stage,\n",
    "        test_size=0.3,\n",
    "        stratify=y_stage,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    print(f\"\\nFinal shapes:\")\n",
    "    print(f\"X_train: {X_train.shape}\")\n",
    "    print(f\"X_test: {X_test.shape}\")\n",
    "    \n",
    "    return X_train, X_test, y_stage_train, y_stage_test, scaler, stage_encoder\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "def create_federated_clients(X_train, y_stage_train, num_clients=5):\n",
    "    \"\"\"Create federated clients using only stage labels.\"\"\"\n",
    "    # Split data into horizontal partitions for federated learning\n",
    "    client_data = np.array_split(X_train, num_clients)\n",
    "    client_stage_labels = np.array_split(y_stage_train, num_clients)\n",
    "    \n",
    "    for i, (data, stage_labels) in enumerate(zip(client_data, client_stage_labels)):\n",
    "        print(f\"\\nClient {i+1}:\")\n",
    "        print(f\"Samples: {len(data)}\")\n",
    "        print(f\"Unique stages: {len(np.unique(stage_labels))}\")\n",
    "    \n",
    "    return client_data, client_stage_labels\n",
    "\n",
    "def main_data_pipeline(data_dir, num_clients=5):\n",
    "    \"\"\"Main preprocessing workflow\"\"\"\n",
    "    combined_df = load_multiple_files(data_dir)\n",
    "    \n",
    "    X_train, X_test, y_stage_train, y_stage_test, scaler, stage_encoder = preprocess_data(combined_df)\n",
    "    \n",
    "    # Create federated clients with only stage labels\n",
    "    client_data, client_stage_labels = create_federated_clients(X_train, y_stage_train, num_clients)\n",
    "    \n",
    "    return client_data, client_stage_labels, X_test, y_stage_test, scaler, stage_encoder\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_directory = \"/home/brahim/Desktop/PFE2025/Dataset/csv/\"\n",
    "    \n",
    "    # Run preprocessing pipeline\n",
    "    client_data, client_stage_labels, X_test, y_stage_test, scaler, stage_encoder = main_data_pipeline(data_directory)\n",
    "    \n",
    "    print(\"\\nPreprocessing complete!\")\n",
    "    print(f\"Training data shape: {sum([c.shape[0] for c in client_data])} samples\")\n",
    "    print(f\"Test data shape: {X_test.shape[0]} samples\")\n",
    "    print(f\"Stage classes: {stage_encoder.classes_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'element_spec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 31\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tff\u001b[38;5;241m.\u001b[39mlearning\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mfrom_keras_model(\n\u001b[1;32m     24\u001b[0m         keras_model,\n\u001b[1;32m     25\u001b[0m         input_spec\u001b[38;5;241m=\u001b[39mclient_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39melement_spec,\n\u001b[1;32m     26\u001b[0m         loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mBinaryCrossentropy(),\n\u001b[1;32m     27\u001b[0m         metrics\u001b[38;5;241m=\u001b[39m[tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mBinaryAccuracy()]\n\u001b[1;32m     28\u001b[0m     )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Build the federated averaging process:\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m iterative_process \u001b[38;5;241m=\u001b[39m \u001b[43mtff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_weighted_fed_avg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mclient_optimizer_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m state \u001b[38;5;241m=\u001b[39m iterative_process\u001b[38;5;241m.\u001b[39minitialize()\n\u001b[1;32m     35\u001b[0m num_rounds \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_federated/python/learning/algorithms/fed_avg.py:197\u001b[0m, in \u001b[0;36mbuild_weighted_fed_avg\u001b[0;34m(model_fn, client_optimizer_fn, server_optimizer_fn, client_weighting, model_distributor, model_aggregator, metrics_aggregator, loop_implementation)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_weights\u001b[38;5;241m.\u001b[39mModelWeights(\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m trainable_weights),\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m non_trainable_weights),\n\u001b[1;32m    192\u001b[0m     )\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;129;43m@tensorflow_computation\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtf_computation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 197\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43minitial_model_weights_fn\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pytype: disable=not-callable\u001b[39;49;00m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVariableModel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_federated/python/core/impl/computation/computation_wrapper.py:495\u001b[0m, in \u001b[0;36mComputationWrapper.__call__.<locals>.<lambda>\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    486\u001b[0m   \u001b[38;5;66;03m# If invoked as a decorator, and with an empty argument list as \"@xyz()\"\u001b[39;00m\n\u001b[1;32m    487\u001b[0m   \u001b[38;5;66;03m# applied to a function definition, expect the Python function being\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    492\u001b[0m   \u001b[38;5;66;03m# The tricky partial recursion is needed to inline the logic in the\u001b[39;00m\n\u001b[1;32m    493\u001b[0m   \u001b[38;5;66;03m# \"success\" case below.\u001b[39;00m\n\u001b[1;32m    494\u001b[0m   provided_types \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 495\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m fn: \u001b[43m_wrap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapper_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovided_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_infer_type_fn\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _is_function(args[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m    499\u001b[0m   \u001b[38;5;66;03m# If the first argument on the list is a Python function, instance method,\u001b[39;00m\n\u001b[1;32m    500\u001b[0m   \u001b[38;5;66;03m# or a tf.function, this is the one that's being wrapped. This is the case\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    504\u001b[0m   \u001b[38;5;66;03m# Any of the following arguments, if present, are the arguments to the\u001b[39;00m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;66;03m# wrapper that are to be interpreted as the type specification.\u001b[39;00m\n\u001b[1;32m    506\u001b[0m   fn \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_federated/python/core/impl/computation/computation_wrapper.py:236\u001b[0m, in \u001b[0;36m_wrap\u001b[0;34m(fn, wrapper_fn, parameter_types, infer_type_fn)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m   \u001b[38;5;66;03m# Either we have a concrete parameter type, or this is no-arg function.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m   parameter_type \u001b[38;5;241m=\u001b[39m _parameter_type(parameters, parameter_types)\n\u001b[0;32m--> 236\u001b[0m   wrapped_fn \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_concrete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrapper_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameter_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# When applying a decorator, the __doc__ attribute with the documentation\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# in triple-quotes is not automatically transferred from the function on\u001b[39;00m\n\u001b[1;32m    240\u001b[0m wrapped_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fn, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__doc__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_federated/python/core/impl/computation/computation_wrapper.py:98\u001b[0m, in \u001b[0;36m_wrap_concrete\u001b[0;34m(fn, wrapper_fn, parameter_type)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m   name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m concrete_fn \u001b[38;5;241m=\u001b[39m \u001b[43mwrapper_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameter_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m py_typecheck\u001b[38;5;241m.\u001b[39mcheck_type(\n\u001b[1;32m    100\u001b[0m     concrete_fn,\n\u001b[1;32m    101\u001b[0m     computation_impl\u001b[38;5;241m.\u001b[39mConcreteComputation,\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue returned by the wrapper\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    103\u001b[0m )\n\u001b[1;32m    104\u001b[0m result_parameter_type \u001b[38;5;241m=\u001b[39m concrete_fn\u001b[38;5;241m.\u001b[39mtype_signature\u001b[38;5;241m.\u001b[39mparameter\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_federated/python/core/environments/tensorflow_frontend/tensorflow_computation.py:79\u001b[0m, in \u001b[0;36m_tf_wrapper_fn\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     74\u001b[0m fn \u001b[38;5;241m=\u001b[39m function_utils\u001b[38;5;241m.\u001b[39mwrap_as_zero_or_one_arg_callable(\n\u001b[1;32m     75\u001b[0m     fn, parameter_type, unpack\n\u001b[1;32m     76\u001b[0m )\n\u001b[1;32m     77\u001b[0m context_stack \u001b[38;5;241m=\u001b[39m context_stack_impl\u001b[38;5;241m.\u001b[39mcontext_stack\n\u001b[1;32m     78\u001b[0m comp_pb, extra_type_spec \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 79\u001b[0m     \u001b[43mtensorflow_serialization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize_py_fn_as_tf_computation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameter_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_stack\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m )\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m computation_impl\u001b[38;5;241m.\u001b[39mConcreteComputation(\n\u001b[1;32m     84\u001b[0m     computation_proto\u001b[38;5;241m=\u001b[39mcomp_pb,\n\u001b[1;32m     85\u001b[0m     context_stack\u001b[38;5;241m=\u001b[39mcontext_stack,\n\u001b[1;32m     86\u001b[0m     annotated_type\u001b[38;5;241m=\u001b[39mextra_type_spec,\n\u001b[1;32m     87\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_federated/python/core/environments/tensorflow_frontend/tensorflow_serialization.py:110\u001b[0m, in \u001b[0;36mserialize_py_fn_as_tf_computation\u001b[0;34m(fn, parameter_type, context_stack)\u001b[0m\n\u001b[1;32m    108\u001b[0m   result \u001b[38;5;241m=\u001b[39m fn(parameter_value)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m computation_wrapper\u001b[38;5;241m.\u001b[39mComputationReturnedNoneError(fn)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_federated/python/core/impl/computation/function_utils.py:446\u001b[0m, in \u001b[0;36mwrap_as_zero_or_one_arg_callable.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parameter_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    443\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m is_signature_compatible_with_types(signature):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;66;03m# Deliberate wrapping to isolate the caller from `fn`, e.g., to prevent\u001b[39;00m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;66;03m# the caller from mistakenly specifying args that match fn's defaults.\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=unnecessary-lambda\u001b[39;00m\n\u001b[1;32m    447\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    449\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m of the supplied function cannot be interpreted as \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma body of a no-parameter computation.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(signature)\n\u001b[1;32m    451\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow_federated/python/learning/algorithms/fed_avg.py:198\u001b[0m, in \u001b[0;36mbuild_weighted_fed_avg.<locals>.initial_model_weights_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;129m@tensorflow_computation\u001b[39m\u001b[38;5;241m.\u001b[39mtf_computation()\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minitial_model_weights_fn\u001b[39m():\n\u001b[0;32m--> 198\u001b[0m   model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pytype: disable=not-callable\u001b[39;00m\n\u001b[1;32m    199\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, variable\u001b[38;5;241m.\u001b[39mVariableModel):\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhen `model_fn` is a callable, it return instances of\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m tff.learning.models.VariableModel. Instead callable returned\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    204\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[35], line 25\u001b[0m, in \u001b[0;36mmodel_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmodel_fn\u001b[39m():\n\u001b[1;32m     22\u001b[0m     keras_model \u001b[38;5;241m=\u001b[39m create_ids_model(input_shape\u001b[38;5;241m=\u001b[39m(num_selected_features,))\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tff\u001b[38;5;241m.\u001b[39mlearning\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mfrom_keras_model(\n\u001b[1;32m     24\u001b[0m         keras_model,\n\u001b[0;32m---> 25\u001b[0m         input_spec\u001b[38;5;241m=\u001b[39m\u001b[43mclient_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melement_spec\u001b[49m,\n\u001b[1;32m     26\u001b[0m         loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mBinaryCrossentropy(),\n\u001b[1;32m     27\u001b[0m         metrics\u001b[38;5;241m=\u001b[39m[tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mBinaryAccuracy()]\n\u001b[1;32m     28\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'element_spec'"
     ]
    }
   ],
   "source": [
    "def create_ids_model(input_shape):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "num_selected_features = client_data[0].shape[1]  # the number of features you decide to use\n",
    "\n",
    "def model_fn():\n",
    "    keras_model = create_ids_model(input_shape=(num_selected_features,))\n",
    "    return tff.learning.models.from_keras_model(\n",
    "        keras_model,\n",
    "        input_spec=client_data[0].element_spec,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "    )\n",
    "\n",
    "# Build the federated averaging process:\n",
    "iterative_process = tff.learning.algorithms.build_weighted_fed_avg(model_fn=model_fn,client_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "\n",
    "state = iterative_process.initialize()\n",
    "\n",
    "num_rounds = 20\n",
    "for round_num in range(1, num_rounds + 1):\n",
    "    state, metrics = iterative_process.next(state, client_data)\n",
    "    print(f'Round {round_num:2d}, Metrics={metrics}')\n",
    "\n",
    "# Evaluate the final global model:\n",
    "global_model = create_ids_model(input_shape=(num_selected_features,))\n",
    "state.model.assign_weights_to(global_model)\n",
    "\n",
    "global_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    ")\n",
    "test_metrics = global_model.evaluate(X_test, y_stage_test, verbose=0)\n",
    "print(f\"\\nTest metrics: {test_metrics}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Initialize clients with their local data\\nclients = {}\\nfor client_id in range(3):  # Example with 3 clients\\n    clients[client_id] = ClientKNN(k=3)\\n    # Each client stores only their local data\\n    clients[client_id].store_data(X_local[client_id], y_local[client_id])\\n\\n# Make federated predictions\\npredictions = simulate_federated_prediction(X_test, clients)\\n\\n# Calculate metrics (can be done either at server or client side)\\nmetrics = {\\n    'accuracy': accuracy_score(y_test, predictions),\\n    'precision': precision_score(y_test, predictions),\\n    'recall': recall_score(y_test, predictions),\\n    'f1_score': f1_score(y_test, predictions)\\n}\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import collections\n",
    "\n",
    "class ClientKNN:\n",
    "    \"\"\"KNN implementation for individual clients\"\"\"\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        \n",
    "    def store_data(self, X, y):\n",
    "        \"\"\"Store local data\"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"Make predictions using only local data\"\"\"\n",
    "        predictions = []\n",
    "        for x in X_test:\n",
    "            distances = np.sqrt(np.sum((self.X - x) ** 2, axis=1))\n",
    "            k_indices = np.argsort(distances)[:self.k]\n",
    "            k_nearest_labels = self.y[k_indices]\n",
    "            prediction = collections.Counter(k_nearest_labels).most_common(1)[0][0]\n",
    "            predictions.append(prediction)\n",
    "        return np.array(predictions)\n",
    "\n",
    "class FederatedServer:\n",
    "    \"\"\"Server that coordinates predictions without storing data\"\"\"\n",
    "    def __init__(self):\n",
    "        self.client_ids = set()\n",
    "    \n",
    "    def register_client(self, client_id):\n",
    "        \"\"\"Register a new client\"\"\"\n",
    "        self.client_ids.add(client_id)\n",
    "    \n",
    "    def aggregate_predictions(self, client_predictions):\n",
    "        \"\"\"Aggregate predictions from all clients using majority voting\"\"\"\n",
    "        all_predictions = np.array(list(client_predictions.values()))\n",
    "        final_predictions = []\n",
    "        \n",
    "        for i in range(all_predictions.shape[1]):\n",
    "            votes = collections.Counter(all_predictions[:, i])\n",
    "            final_predictions.append(votes.most_common(1)[0][0])\n",
    "            \n",
    "        return np.array(final_predictions)\n",
    "\n",
    "def simulate_federated_prediction(X_test, clients):\n",
    "    \"\"\"Simulate the federated prediction process\"\"\"\n",
    "    server = FederatedServer()\n",
    "    client_predictions = {}\n",
    "    \n",
    "    # Each client makes predictions locally\n",
    "    for client_id, client in clients.items():\n",
    "        server.register_client(client_id)\n",
    "        predictions = client.predict(X_test)\n",
    "        client_predictions[client_id] = predictions\n",
    "    \n",
    "    # Server aggregates results without seeing the actual data\n",
    "    final_predictions = server.aggregate_predictions(client_predictions)\n",
    "    return final_predictions\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# Initialize clients with their local data\n",
    "clients = {}\n",
    "for client_id in range(3):  # Example with 3 clients\n",
    "    clients[client_id] = ClientKNN(k=3)\n",
    "    # Each client stores only their local data\n",
    "    clients[client_id].store_data(X_local[client_id], y_local[client_id])\n",
    "\n",
    "# Make federated predictions\n",
    "predictions = simulate_federated_prediction(X_test, clients)\n",
    "\n",
    "# Calculate metrics (can be done either at server or client side)\n",
    "metrics = {\n",
    "    'accuracy': accuracy_score(y_test, predictions),\n",
    "    'precision': precision_score(y_test, predictions),\n",
    "    'recall': recall_score(y_test, predictions),\n",
    "    'f1_score': f1_score(y_test, predictions)\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def balance_network_datasets(normal_file, apt_file, output_file1, output_file2):\n",
    "    \"\"\"\n",
    "    Combine and balance network traffic datasets\n",
    "    \n",
    "    Parameters:\n",
    "    normal_file: path to file containing normal traffic\n",
    "    apt_file: path to file containing APT traffic\n",
    "    output_file1: path for first balanced output file\n",
    "    output_file2: path for second balanced output file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the datasets\n",
    "    print(\"Reading input files...\")\n",
    "    normal_df = pd.read_csv(normal_file)\n",
    "    apt_df = pd.read_csv(apt_file)\n",
    "    \n",
    "    # Add labels if they don't exist\n",
    "    normal_df['label'] = 0  # 0 for normal traffic\n",
    "    apt_df['label'] = 1     # 1 for APT traffic\n",
    "    \n",
    "    # Calculate sizes for balanced datasets\n",
    "    min_normal = len(normal_df)\n",
    "    min_apt = len(apt_df)\n",
    "    \n",
    "    # Determine samples needed per class for 50-50 split\n",
    "    samples_per_class = min(min_normal, min_apt)\n",
    "    \n",
    "    print(f\"Original dataset sizes:\")\n",
    "    print(f\"Normal traffic: {min_normal} samples\")\n",
    "    print(f\"APT traffic: {min_apt} samples\")\n",
    "    print(f\"Will use {samples_per_class} samples per class for balance\")\n",
    "    \n",
    "    # Randomly sample equal amounts from each dataset\n",
    "    if min_normal > samples_per_class:\n",
    "        normal_df = normal_df.sample(n=samples_per_class, random_state=42)\n",
    "    if min_apt > samples_per_class:\n",
    "        apt_df = apt_df.sample(n=samples_per_class, random_state=42)\n",
    "    \n",
    "    # Combine the datasets\n",
    "    combined_df = pd.concat([normal_df, apt_df], ignore_index=True)\n",
    "    \n",
    "    # Shuffle the combined dataset\n",
    "    combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Split into two equal parts\n",
    "    df1, df2 = train_test_split(combined_df, test_size=0.5, stratify=combined_df['label'], random_state=42)\n",
    "    \n",
    "    # Verify balance in each dataset\n",
    "    print(\"\\nDataset 1 distribution:\")\n",
    "    print(df1['label'].value_counts(normalize=True))\n",
    "    print(\"\\nDataset 2 distribution:\")\n",
    "    print(df2['label'].value_counts(normalize=True))\n",
    "    \n",
    "    # Save to files\n",
    "    df1.to_csv(output_file1, index=False)\n",
    "    df2.to_csv(output_file2, index=False)\n",
    "    !\n",
    "    print(f\"\\nSaved balanced datasets:\")\n",
    "    print(f\"Dataset 1: {len(df1)} samples saved to {output_file1}\")\n",
    "    print(f\"Dataset 2: {len(df2)} samples saved to {output_file2}\")\n",
    "    \n",
    "    return df1, df2\n",
    "\n",
    "def verify_balance(df1, df2):\n",
    "    \"\"\"Verify that the datasets are properly balanced\"\"\"\n",
    "    \n",
    "    print(\"\\nBalance Verification:\")\n",
    "    print(\"\\nDataset 1:\")\n",
    "    print(f\"Total samples: {len(df1)}\")\n",
    "    print(\"Class distribution:\")\n",
    "    print(df1['label'].value_counts(normalize=True))\n",
    "    \n",
    "    print(\"\\nDataset 2:\")\n",
    "    print(f\"Total samples: {len(df2)}\")\n",
    "    print(\"Class distribution:\")\n",
    "    print(df2['label'].value_counts(normalize=True))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # File paths - replace with your actual file paths\n",
    "    normal_traffic_file = \"normal_traffic.csv\"\n",
    "    apt_traffic_file = \"apt_traffic.csv\"\n",
    "    output_dataset1 = \"balanced_dataset1.csv\"\n",
    "    output_dataset2 = \"balanced_dataset2.csv\"\n",
    "    \n",
    "    # Process the datasets\n",
    "    df1, df2 = balance_network_datasets(\n",
    "        normal_traffic_file,\n",
    "        apt_traffic_file,\n",
    "        output_dataset1,\n",
    "        output_dataset2\n",
    "    )\n",
    "    \n",
    "    # Verify the balance\n",
    "    verify_balance(df1, df2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
